{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m68lMHXEnHOD"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install flash-attn --no-build-isolation\n",
        "!huggingface-cli download jinaai/jina-embeddings-v3 --local-dir ./models/jina-embeddings-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IeYyMRj-nGA6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- configuration_xlm_roberta.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "291f0779a9884d0d93ecf911178f28d3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_lora.py:   0%|          | 0.00/15.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d81548bb5b494169b390140315015eb5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modeling_xlm_roberta.py:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- stochastic_depth.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- rotary.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- mha.py\n",
            "- rotary.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- mlp.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- block.py\n",
            "- stochastic_depth.py\n",
            "- mha.py\n",
            "- mlp.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- embedding.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- xlm_padding.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- modeling_xlm_roberta.py\n",
            "- block.py\n",
            "- embedding.py\n",
            "- xlm_padding.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/jinaai/xlm-roberta-flash-implementation:\n",
            "- modeling_lora.py\n",
            "- modeling_xlm_roberta.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "/home/ethan/miniconda3/envs/song-rec/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:985: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  def forward(\n",
            "/home/ethan/miniconda3/envs/song-rec/lib/python3.10/site-packages/flash_attn/ops/triton/layer_norm.py:1044: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  def backward(ctx, dout, *args):\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda import is_available as is_cuda_available\n",
        "from transformers import AutoModel\n",
        "\n",
        "# from xlm_roberta.modeling_lora import XLMRobertaLoRA\n",
        "\n",
        "model_folder = './models/jina-embeddings-v3/'\n",
        "\n",
        "# Initialize the model\n",
        "model = AutoModel.from_pretrained(model_folder, trust_remote_code=True, use_flash_attn=False)\n",
        "# model: XLMRobertaLoRA\n",
        "\n",
        "if is_cuda_available():\n",
        "    model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wcHnagpnGA7",
        "outputId": "1381a028-23b7-4681-aeb1-c5da08ac3b1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 1024)\n",
            "[ 0.01671595 -0.05356369  0.07552624 ...  0.00800042 -0.02177768\n",
            " -0.00731576]\n"
          ]
        }
      ],
      "source": [
        "texts = [\n",
        "    # \"sample text\",\n",
        "    \"Look at her face\",\n",
        "    \"a love song\",\n",
        "    \"a sad song\",\n",
        "]\n",
        "\n",
        "# When calling the `encode` function, you can choose a `task` based on the use case:\n",
        "# 'retrieval.query', 'retrieval.passage', 'separation', 'classification', 'text-matching'\n",
        "# Alternatively, you can choose not to pass a `task`, and no specific LoRA adapter will be used.\n",
        "query_embeddings = model.encode(texts, task=\"retrieval.query\")\n",
        "print(query_embeddings.shape)\n",
        "print(query_embeddings[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3CNQ_xEnGA7",
        "outputId": "6288986b-b1d1-4178-ce67-b151b1477a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(57650, 1024)\n",
            "[-0.1406054  -0.04126596  0.02898028 ... -0.0118162  -0.0104178\n",
            " -0.00558778]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# load embeddings\n",
        "passage_embeddings = np.load('./embeddings/jina-embeddings-v3_retrieval.passage.npy')\n",
        "print(passage_embeddings.shape)\n",
        "print(passage_embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8Rih86er5FS"
      },
      "source": [
        "1. Multi-threaded parallel MapReduce\n",
        "This approach utilizes Python's ThreadPoolExecutor to slice the similarity computation task and process it in parallel:\n",
        "Slicing (Map): The large-scale embedded matrix is sliced by rows into a number of small blocks, and each block calculates the similarity individually.\n",
        "Parallel Computing: Process multiple slices simultaneously through multiple threads, making full use of multi-core CPUs to improve computational efficiency.\n",
        "Reduce: Splice the similarity results of each slice into a complete similarity matrix.\n",
        "\n",
        "Applicable Scenarios:\n",
        "The embedding matrix is large and the memory of a single machine is not enough to load all the data at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBSLPrTInGA7",
        "outputId": "d0f772f8-c8c4-4e87-94f8-4b8db1b3d790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MapReduce-style computation completed!\n",
            "[[0.30582494 0.11828539 0.15932773 ... 0.0956329  0.0874272  0.08308057]\n",
            " [0.30726215 0.35276058 0.36984026 ... 0.30388635 0.27699167 0.34673175]\n",
            " [0.32911277 0.30546767 0.3108574  ... 0.22833987 0.30470648 0.32797337]]\n",
            "CPU times: user 179 ms, sys: 246 ms, total: 426 ms\n",
            "Wall time: 16.5 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_shard_similarity(shard, query_embeddings):\n",
        "    return query_embeddings @ shard.T\n",
        "\n",
        "# divide `passage_embeddings` into different pieces\n",
        "num_shards = 10\n",
        "shards = np.array_split(passage_embeddings, num_shards, axis=0)\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    shard_similarities = list(executor.map(compute_shard_similarity, shards, [query_embeddings] * len(shards)))\n",
        "\n",
        "\n",
        "similarities = np.hstack(shard_similarities)\n",
        "print(\"MapReduce-style computation completed!\")\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xodea6anr31p"
      },
      "source": [
        "2. MapReduce based on map and functools.reduce\n",
        "This approach simulates the MapReduce workflow using Python's built-in map and functools.reduce:\n",
        "Map: Each slice of the embedded matrix is passed into the mapper function, which computes the similarity.\n",
        "Reduce: Merge the results of all the slices with functools.reduce to get the full similarity matrix.\n",
        "\n",
        "Applicable Scenarios:\n",
        "Embedded matrices are moderate and only need to be processed in a standalone environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWMjsVXI-Gz8",
        "outputId": "9a3b198c-63e8-4c31-b598-f28df59a7609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MapReduce-style computation completed!\n",
            "[[0.30582494 0.11828539 0.15932773 ... 0.0956329  0.0874272  0.08308057]\n",
            " [0.30726215 0.35276058 0.36984026 ... 0.30388635 0.27699167 0.34673175]\n",
            " [0.32911277 0.30546767 0.3108574  ... 0.22833987 0.30470648 0.32797337]]\n",
            "CPU times: user 227 ms, sys: 43.6 ms, total: 271 ms\n",
            "Wall time: 8.79 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import functools\n",
        "import numpy as np\n",
        "\n",
        "# define mapper 和 reducer\n",
        "def mapper(shard):\n",
        "    shard_similarity = query_embeddings @ shard.T\n",
        "    return shard_similarity\n",
        "\n",
        "def reducer(p, c):\n",
        "    return np.hstack((p, c))\n",
        "\n",
        "# divide `passage_embeddings` into many pieces\n",
        "num_shards = 10\n",
        "shards = np.array_split(passage_embeddings, num_shards, axis=0)\n",
        "\n",
        "mapped = map(mapper, shards)\n",
        "reduced = functools.reduce(reducer, mapped)\n",
        "\n",
        "print(\"MapReduce-style computation completed!\")\n",
        "print(reduced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuUhO8Xsmdj"
      },
      "source": [
        "Comparison Summary\n",
        "Multi-threaded parallel MapReduce realizes parallel processing through thread pooling, which is more suitable for super-large-scale embedded matrix processing, especially in multi-core CPU environment, which can give full play to the hardware performance.\n",
        "\n",
        "MapReduce based on map and functools.reduce is more lightweight and suitable for medium-sized data processing, but performs better when computing resources are limited or the task size is small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.30582494 0.11828539 0.15932773 ... 0.0956329  0.0874272  0.08308057]\n",
            " [0.30726215 0.35276058 0.36984026 ... 0.30388635 0.27699167 0.34673175]\n",
            " [0.32911277 0.30546767 0.3108574  ... 0.22833987 0.30470648 0.32797337]]\n",
            "CPU times: user 269 ms, sys: 102 ms, total: 370 ms\n",
            "Wall time: 12.2 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Compute similarities\n",
        "similarities = query_embeddings @ passage_embeddings.T\n",
        "print(similarities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je0FEtFNnGA7",
        "outputId": "fef4fdc1-44ac-40b9-b2a7-963a376ddadc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[43615  4627 33356 20269 11272]\n",
            " [25317  1841 31527 44260  6152]\n",
            " [23647 32184 45343  4425 33847]]\n",
            "CPU times: user 16.5 ms, sys: 298 μs, total: 16.8 ms\n",
            "Wall time: 16.7 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# get top k indices\n",
        "top_k = 5\n",
        "top_k_indices = np.argsort(-similarities, axis=1)[:, :top_k]\n",
        "print(top_k_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEoZEMunnGA8",
        "outputId": "e11b0c0f-4fea-489c-e86d-a5e992abe0b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[43615  4627 33356 20269 11272]\n",
            " [25317  1841 31527 44260  6152]\n",
            " [23647 32184 45343  4425 33847]]\n",
            "CPU times: user 3.42 ms, sys: 534 μs, total: 3.96 ms\n",
            "Wall time: 4.05 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# a more efficient way to get top k indices\n",
        "num_queries = similarities.shape[0]\n",
        "arange = np.arange(num_queries)[:, None]\n",
        "\n",
        "top_k = 5\n",
        "top_k_indices = np.argpartition(-similarities, top_k, axis=1)[:, :top_k]\n",
        "# Sort the top_k indices to get them in order\n",
        "top_k_indices = top_k_indices[arange, np.argsort(-similarities[arange, top_k_indices])]\n",
        "print(top_k_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tJ04XwAnGA8",
        "outputId": "0e1fadf6-1cbc-474e-906b-908c4fb3fd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.526491   0.5235444  0.5057533  0.49374926 0.4898193 ]\n",
            "[0.5771628  0.5733152  0.5681341  0.5577022  0.55689794]\n",
            "[0.6037626  0.5811384  0.57794267 0.5760369  0.55670303]\n"
          ]
        }
      ],
      "source": [
        "# print similarities for top k indices\n",
        "for i in range(len(top_k_indices)):\n",
        "    print(similarities[i, top_k_indices[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiUYQjlwnGA8"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97YwIjvPnGA8",
        "outputId": "96dc05d1-d79b-4506-ff7d-cc833e297909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  artist                   song                                        link  \\\n",
            "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
            "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
            "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
            "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
            "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
            "\n",
            "                                                text  \n",
            "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
            "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
            "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
            "3  Making somebody happy is a question of give an...  \n",
            "4  Making somebody happy is a question of give an...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = './data/spotify_millsongdata.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h2IIv9SlnGA8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               artist                   song  \\\n",
            "25317  Billie Holiday  Lover Come Back To Me   \n",
            "1841     Bonnie Raitt          Cure For Love   \n",
            "31527  Emmylou Harris                Love Is   \n",
            "44260  Modern Talking      Romantic Warriors   \n",
            "6152    Fleetwood Mac         It's Only Love   \n",
            "\n",
            "                                                    link  \\\n",
            "25317  /b/billie+holiday/lover+come+back+to+me_200180...   \n",
            "1841         /b/bonnie+raitt/cure+for+love_20022697.html   \n",
            "31527            /e/emmylou+harris/love+is_20050047.html   \n",
            "44260  /m/modern+talking/romantic+warriors_20094699.html   \n",
            "6152        /f/fleetwood+mac/its+only+love_20632943.html   \n",
            "\n",
            "                                                    text  \n",
            "25317  The sky was blue  \\r\\nAnd high above  \\r\\nThe ...  \n",
            "1841   You bring me roses  \\r\\nYou give me kisses  \\r...  \n",
            "31527  Love is a shiny car  \\r\\nLove is a steel guita...  \n",
            "44260  In the nights of lost and found  \\r\\nMany stra...  \n",
            "6152   I think I met my match again  \\r\\nPassing 'rou...  \n"
          ]
        }
      ],
      "source": [
        "# get entries for top k indices of query \"a love song\"\n",
        "top_k_entries = df.iloc[top_k_indices[-2]]\n",
        "print(top_k_entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "MxkdyumYnGA8",
        "outputId": "a1655a69-cde9-48c3-cd69-98b610537cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              artist           song  \\\n",
            "23647  Alison Krauss  This Sad Song   \n",
            "32184       Everlast       Sad Girl   \n",
            "45343    Neil Sedaka  Sad Sad Story   \n",
            "4425    Donna Summer     Sing Along   \n",
            "33847  George Strait  Blue Melodies   \n",
            "\n",
            "                                               link  \\\n",
            "23647  /a/alison+krauss/this+sad+song_20521716.html   \n",
            "32184            /e/everlast/sad+girl_20181664.html   \n",
            "45343    /n/neil+sedaka/sad+sad+story_20613341.html   \n",
            "4425       /d/donna+summer/sing+along_10087839.html   \n",
            "33847  /g/george+strait/blue+melodies_21061476.html   \n",
            "\n",
            "                                                    text  \n",
            "23647  Well, the rain is apourin' down in a fury  \\r\\...  \n",
            "32184  I seen her at a stop light on Alverano  \\r\\nWa...  \n",
            "45343  Look at the lady she's the one with the broken...  \n",
            "4425   I've an emptiness inside  \\r\\nThat can only be...  \n",
            "33847  I don't know how to write you a song  \\r\\nThat...  \n"
          ]
        }
      ],
      "source": [
        "# get entries for top k indices of query \"a sad song\"\n",
        "top_k_entries = df.iloc[top_k_indices[-1]]\n",
        "print(top_k_entries)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
